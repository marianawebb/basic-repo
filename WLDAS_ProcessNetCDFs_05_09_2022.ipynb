{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marianawebb/basic-repo/blob/main/WLDAS_ProcessNetCDFs_05_09_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csw1hH50JLnF",
        "outputId": "6daa62af-ff2e-4a9f-c059-c07a25936cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfswckB2TtQF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Update spatial packages from colab default\n",
        "!mamba install -q -c conda-forge cartopy\n",
        "!mamba install -q -c conda-forge geopandas\n",
        "!mamba install -q -c conda-forge rioxarray\n",
        "# !mamba install -q -c conda-forge nc-time-axis\n",
        "!mamba install -q -c conda-forge regionmask\n",
        "!mamba install -q -c conda-forge dask \n",
        "\n",
        "# !pip install rioxarray\n",
        "# !pip install geopandas\n",
        "# !pip install regionmask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LmR0xJF2S-0J",
        "outputId": "46736034-ab21-4109-91c4-84377feda3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Dissertation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import cartopy.crs as ccrs\n",
        "import cartopy.feature as cfeature\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import regionmask\n",
        "import xarray as xr\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rioxarray as rio\n",
        "import csv\n",
        "import dask\n",
        "from dask.diagnostics import ProgressBar\n",
        "from numba import jit, njit, vectorize, cuda\n",
        " \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('/content/drive/My Drive/Dissertation/')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6Q8Y31WBeqV"
      },
      "outputs": [],
      "source": [
        "def preprocessNetCDFs(raw_nc):\n",
        "  print(raw_nc.time.values[0])\n",
        "  # add lat and long as coordinates for WLDAS xarrays\n",
        "  file = open(\"Data/WLDAS_LatLon/WLDAS_Lats.csv\", \"r\")\n",
        "  csv_reader = csv.reader(file)\n",
        "  lats = []\n",
        "  for row in csv_reader:\n",
        "       lats.append(row[0])\n",
        "  lats = lats[1:]\n",
        "  lats = [float(i) for i in lats]\n",
        "\n",
        "  file = open(\"Data/WLDAS_LatLon/WLDAS_Lons.csv\", \"r\")\n",
        "  csv_reader = csv.reader(file)\n",
        "  lons = []\n",
        "  for row in csv_reader:\n",
        "       lons.append(row[0])\n",
        "  lons = lons[1:]\n",
        "  lons = [float(i) for i in lons]\n",
        "\n",
        "  # drop_nc = raw_nc.drop(['Swnet_tavg','Lwnet_tavg','Qle_tavg','Qh_tavg','Qg_tavg','VegT_tavg','BareSoilT_tavg','RadT_tavg','ECanop_tavg','TVeg_tavg','ESoil_tavg','WaterTableD_tavg','Wind_f_tavg','Rainf_f_tavg','Psurf_f_tavg','SWdown_f_tavg','LWdown_f_tavg'])\n",
        "  drop_nc = raw_nc.drop(['Swnet_tavg', 'Lwnet_tavg', 'Qle_tavg', 'Qh_tavg', 'Qg_tavg', 'VegT_tavg', 'BareSoilT_tavg', 'AvgSurfT_tavg', 'RadT_tavg', 'SnowDepth_tavg', 'SoilTemp_tavg', 'TVeg_tavg', 'ESoil_tavg', 'SubSnow_tavg', 'WaterTableD_tavg', 'SnowCover_tavg', 'Rainf_f_tavg', 'Psurf_f_tavg', 'SWdown_f_tavg', 'LWdown_f_tavg'])\n",
        "\n",
        "\n",
        "  coord_nc = drop_nc.assign(north_south=lats, east_west=lons).drop(['lat', 'lon'])\n",
        "  coord_nc = coord_nc.rename({'north_south':'lat', 'east_west':'lon'}).rio.write_crs(\"epsg:4326\", inplace=True) ##assuming WGS84???\n",
        "\n",
        "\n",
        "  #crop to lat lon bounds of watersheds\n",
        "  crop_nc = coord_nc.sel(\n",
        "       lon = slice(-124.5, -117.5),\n",
        "       lat = slice(34.0, 49.0)\n",
        "  )\n",
        "\n",
        "\n",
        "  # make each soil moisture depth into a separate data variable\n",
        "  soil_moist = crop_nc.SoilMoist_tavg\n",
        "  soil_moist_list = []\n",
        "  for i in np.arange(0,4):\n",
        "    temp = soil_moist.isel({'SoilMoist_profiles':i})\n",
        "    temp = temp.rename(temp.name + \"_\" + str(i+1))\n",
        "    soil_moist_list.append(temp)\n",
        "\n",
        "  soil_moist_list[0]\n",
        "  soil_moist_merge = xr.merge(soil_moist_list)\n",
        "\n",
        "  drop_nc = crop_nc.drop(['SoilMoist_tavg'])\n",
        "  nc = xr.merge([drop_nc, soil_moist_merge])\n",
        "\n",
        "  return nc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFmiCfIOC7W9",
        "outputId": "74c57cd1-9450-4661-e29e-cbde68875b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01\n",
            "200001 already processed\n",
            "02\n",
            "200002 already processed\n",
            "03\n",
            "200003 already processed\n",
            "04\n",
            "200004 already processed\n",
            "05\n",
            "200005 already processed\n",
            "06\n",
            "200006 already processed\n",
            "07\n",
            "200007 already processed\n",
            "08\n",
            "200008 already processed\n",
            "09\n",
            "200009 already processed\n",
            "10\n",
            "200010 already processed\n",
            "11\n",
            "200011 already processed\n",
            "12\n",
            "200012 already processed\n",
            "01\n",
            "200101 already processed\n",
            "02\n",
            "200102 already processed\n",
            "03\n",
            "200103 already processed\n",
            "04\n",
            "200104 already processed\n",
            "05\n",
            "200105 already processed\n",
            "06\n",
            "200106 already processed\n",
            "07\n",
            "200107 already processed\n",
            "08\n",
            "200108 already processed\n",
            "09\n",
            "200109 already processed\n",
            "10\n",
            "200110 already processed\n",
            "11\n",
            "200111 already processed\n",
            "12\n",
            "200112 already processed\n",
            "01\n",
            "200201 already processed\n",
            "02\n",
            "200202 already processed\n",
            "03\n",
            "200203 already processed\n",
            "04\n",
            "200204 already processed\n",
            "05\n",
            "200205 already processed\n",
            "06\n",
            "200206 already processed\n",
            "07\n",
            "200207 already processed\n",
            "08\n",
            "200208 already processed\n",
            "09\n",
            "200209 already processed\n",
            "10\n",
            "200210 already processed\n",
            "11\n",
            "200211 already processed\n",
            "12\n",
            "200212 already processed\n",
            "01\n",
            "200301 already processed\n",
            "02\n",
            "200302 already processed\n",
            "03\n",
            "200303 already processed\n",
            "04\n",
            "200304 already processed\n",
            "05\n",
            "200305 already processed\n",
            "06\n",
            "200306 already processed\n",
            "07\n",
            "200307 already processed\n",
            "08\n",
            "200308 already processed\n",
            "09\n",
            "2003-09-01T00:00:00.000000000\n",
            "masking...\n",
            "calculating watershed means...\n",
            "calculating watershed sums...\n",
            "merging into full dataframe\n",
            "computing dask...\n",
            "[########################################] | 100% Completed | 26min  9.5s\n",
            "exporting...\n",
            "exported data for 2003\n",
            "10\n",
            "2003-10-01T00:00:00.000000000\n",
            "masking...\n",
            "calculating watershed means...\n",
            "calculating watershed sums...\n",
            "merging into full dataframe\n",
            "computing dask...\n",
            "[#######################                 ] | 57% Completed | 15min 52.5s"
          ]
        }
      ],
      "source": [
        "# set year of interest\n",
        "# yr = 1980\n",
        "yr_list = np.arange(2000,2016)\n",
        "\n",
        "# for loop for 5 years of data\n",
        "for yr in yr_list:\n",
        "\n",
        "  # load in shapefiles for each watershed\n",
        "  watersheds_shp = gpd.read_file(\"Data/Shapefiles/watershed_boundaries.shp\") \n",
        "  watersheds_shp['GAGE_ID'] = watersheds_shp['GAGE_ID'].astype('int64')\n",
        "\n",
        "\n",
        "  #create a list containing a gdp object for each watershed\n",
        "  watersheds_shp_list = []\n",
        "  for i in watersheds_shp.iterrows():\n",
        "    watersheds_shp_list.append(i)\n",
        "\n",
        "\n",
        "  # load in list of one year of WLDAS netCDF\n",
        "  months_list = [str(x).zfill(2) for x in np.arange(1,13)]\n",
        "\n",
        "  for month in months_list:\n",
        "    print(month)\n",
        "    WLDAS_filelist = glob.glob('Data/WLDAS_Full/'+str(yr)+'/LIS_HIST_'+str(yr)+month+'*.nc')\n",
        "\n",
        "    # checking if year already processed\n",
        "    ym = str(yr)+month\n",
        "    processed_files = glob.glob(\"Data/WLDAS_Processed/*.csv\") + glob.glob(\"Data/WLDAS_ProcessedUNR/*.csv\")\n",
        "    processed_ym = [x[-10:-4] for x in processed_files]\n",
        "    if ym in processed_ym:\n",
        "      print(ym+\" already processed\")\n",
        "      continue\n",
        "\n",
        "    # open netCDFs at xarray objects\n",
        "    nc = xr.open_mfdataset(WLDAS_filelist, parallel=False) #this method creates a dask array which I don't know how to work with :(\n",
        "    nc = preprocessNetCDFs(nc)\n",
        "\n",
        "    # create watersheds mask\n",
        "    watersheds_mask = regionmask.mask_3D_geopandas(watersheds_shp, nc, numbers = \"GAGE_ID\", overlap = True)\n",
        "\n",
        "\n",
        "    # mask WLDAS data to watershed extents\n",
        "    print(\"masking...\")\n",
        "    masked_WLDAS = nc.where(watersheds_mask)\n",
        "\n",
        "\n",
        "    #calculate cummulative and mean values for each watershed for each day\n",
        "    print(\"calculating watershed means...\")\n",
        "    WLDAS_means = masked_WLDAS.groupby(\"region\").mean(['lat', 'lon'])\n",
        "    means_df = WLDAS_means.to_dask_dataframe()\n",
        "    means_df.columns = means_df.columns.str.replace('_tavg', '')\n",
        "    means_df = means_df.rename(columns=dict(zip(means_df.columns[3:], means_df.columns[3:] + '_mean')))\n",
        "    # means_df = means_df.iloc[:,:-1]\n",
        "\n",
        "    print(\"calculating watershed sums...\")\n",
        "    WLDAS_sums = masked_WLDAS.groupby(\"region\").sum(['lat', 'lon'])\n",
        "    sums_df = WLDAS_sums.to_dask_dataframe()\n",
        "    sums_df.columns = sums_df.columns.str.replace('_tavg', '')\n",
        "    sums_df = sums_df.rename(columns=dict(zip(sums_df.columns[3:], sums_df.columns[3:] + '_sum')))\n",
        "    # sums_df = sums_df.iloc[:,:-1]\n",
        "\n",
        "\n",
        "    # merge sum and mean dataframes\n",
        "    print(\"merging into full dataframe\")\n",
        "    full_df = means_df.merge(\n",
        "        sums_df, \n",
        "        how=\"left\", \n",
        "        on=['region', 'time', 'spatial_ref']\n",
        "    )\n",
        "\n",
        "    print(\"computing dask...\")\n",
        "    with ProgressBar():\n",
        "      compute_df = full_df.compute()\n",
        "\n",
        "    print(\"exporting...\")\n",
        "    export_location = 'Data/WLDAS_ProcessedUNR/' + 'WLDAS_' +str(yr)+month+ '.csv'\n",
        "    compute_df.to_csv(export_location, index = True)\n",
        "    print('exported data for '+ str(yr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPxYCcxZLsqC"
      },
      "source": [
        "Using these tutorials to help with making the multi-polygon mask\n",
        "http://www.matteodefelice.name/post/aggregating-gridded-data/\n",
        "https://www.earthdatascience.org/courses/use-data-open-source-python/hierarchical-data-formats-hdf/subset-netcdf4-climate-data-spatially-aoi/\n",
        "https://www.guillaumedueymes.com/post/shapefiles_country/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K0dC2AnZwZH"
      },
      "source": [
        "For info on how the vector watershed outlines are rasterized: https://regionmask.readthedocs.io/en/stable/notebooks/method.html\n",
        "\n",
        "Basically, if the 1 km centroid falls in the watershed, we include it in the mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwy140QQlvhP",
        "outputId": "10d274bb-64b2-46f6-fa4d-35d16ad5ad4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "906224"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sys.getsizeof(compute_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "WLDAS_ProcessNetCDFs_05_09_2022.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}